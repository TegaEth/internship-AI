{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPe9/UoXEzm+tqxlg0fAlLk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TegaEth/internship-AI/blob/main/internship_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSlvFPzbEQ8B",
        "outputId": "ce6ddeb4-d481-47a0-bdbe-c5d33f51a7e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.4.8-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.84.0)\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.17.2-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.6.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.63)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.44)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting langgraph-checkpoint>=2.0.26 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.26-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt>=0.2.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.70-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.13.4)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting dataclasses-json (from unstructured)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.0.2)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.36.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.17.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (11.2.1)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading XlsxWriter-3.2.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint>=2.0.26->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured) (2.7)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->unstructured)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->unstructured)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (2024.11.6)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (24.1.0)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (43.0.3)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Downloading langgraph-0.4.8-py3-none-any.whl (152 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured-0.17.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.6.0-py3-none-any.whl (304 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.26-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.2.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.70-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading XlsxWriter-3.2.3-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.36.0-py3-none-any.whl (195 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.8/195.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=9b7d7bd5a49f1d228a1ac7dedb2d01d43b2c1ac35e1f0835fc51164b4293dad7\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, XlsxWriter, rapidfuzz, python-magic, python-iso639, pypdf, ormsgpack, olefile, mypy-extensions, marshmallow, langdetect, emoji, backoff, typing-inspect, python-pptx, python-oxmsg, unstructured-client, langgraph-sdk, dataclasses-json, unstructured, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed XlsxWriter-3.2.3 backoff-2.2.1 dataclasses-json-0.6.7 emoji-2.14.1 filetype-1.2.0 langdetect-1.0.9 langgraph-0.4.8 langgraph-checkpoint-2.0.26 langgraph-prebuilt-0.2.2 langgraph-sdk-0.1.70 marshmallow-3.26.1 mypy-extensions-1.1.0 olefile-0.47 ormsgpack-1.10.0 pypdf-5.6.0 python-iso639-2025.2.18 python-magic-0.4.27 python-oxmsg-0.0.2 python-pptx-1.0.2 rapidfuzz-3.13.0 typing-inspect-0.9.0 unstructured-0.17.2 unstructured-client-0.36.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langgraph openai unstructured pypdf python-pptx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "98Uv42QzFOJV",
        "outputId": "4925698b-5cfe-493a-b66b-6f4db13ef17a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.65 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.44)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (2.11.5)\n",
            "Collecting langsmith<0.4,>=0.1.125 (from langchain-community)\n",
            "  Downloading langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-community) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.25-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.65-py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, httpx-sse, pydantic-settings, langsmith, langchain-core, langchain-community\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.44\n",
            "    Uninstalling langsmith-0.3.44:\n",
            "      Successfully uninstalled langsmith-0.3.44\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.63\n",
            "    Uninstalling langchain-core-0.3.63:\n",
            "      Successfully uninstalled langchain-core-0.3.63\n",
            "Successfully installed httpx-sse-0.4.0 langchain-community-0.3.25 langchain-core-0.3.65 langsmith-0.3.45 pydantic-settings-2.9.1 python-dotenv-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "langchain_core"
                ]
              },
              "id": "4baaf9006b4241ccb71880a045d51d64"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_cerebras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRl9HY8SH6jj",
        "outputId": "4716c1c1-aca9-4a5a-bf73-d9424bf77044"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_cerebras\n",
            "  Downloading langchain_cerebras-0.5.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.11/dist-packages (from langchain_cerebras) (0.3.65)\n",
            "Collecting langchain-openai<0.4.0,>=0.3.0 (from langchain_cerebras)\n",
            "  Downloading langchain_openai-0.3.23-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (0.3.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (4.14.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (2.11.5)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai<0.4.0,>=0.3.0->langchain_cerebras) (1.84.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai<0.4.0,>=0.3.0->langchain_cerebras) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai<0.4.0,>=0.3.0->langchain_cerebras) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai<0.4.0,>=0.3.0->langchain_cerebras) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai<0.4.0,>=0.3.0->langchain_cerebras) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai<0.4.0,>=0.3.0->langchain_cerebras) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai<0.4.0,>=0.3.0->langchain_cerebras) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (0.4.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai<0.4.0,>=0.3.0->langchain_cerebras) (2024.11.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai<0.4.0,>=0.3.0->langchain_cerebras) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.29->langchain_cerebras) (2.4.0)\n",
            "Downloading langchain_cerebras-0.5.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_openai-0.3.23-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai, langchain_cerebras\n",
            "Successfully installed langchain-openai-0.3.23 langchain_cerebras-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fpdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLm_e0EwH_ga",
        "outputId": "8372e195-a6c2-4895-bda2-c2c2deb6c746"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=602aacbe46d7e06a232e267550bafcf073808df4afe3b07ecf428dc47e9cce34\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir())  # Should show your uploaded file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvxhjSFTJ9Fm",
        "outputId": "0ade42f2-0670-410f-dee8-ed2ef8d8bbfd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', '4.pdf', '5.pdf', '.git', '.ipynb_checkpoints', '6.pdf', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader, UnstructuredPowerPointLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.runnables import RunnableLambda, RunnableMap\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_cerebras import ChatCerebras\n",
        "from fpdf import FPDF\n",
        "import os\n",
        "import random\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "r-qXF2YxFKxe"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#llmm to be used\n",
        "llm = ChatCerebras(\n",
        "    model=\"llama-3.3-70b\",\n",
        "    api_key=\"csk-ph49yn4383r2rrhh6ch4kjrhddfjd3frt98npmecmd5dknf5\"\n",
        ")"
      ],
      "metadata": {
        "id": "knFTB6RDGgDk"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Node 1: Upload Handler (Multiple PDF or PPTX files)\n",
        "\n",
        "def load_documents(file_paths: list[str]) -> str:\n",
        "    all_text = []\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            if file_path.endswith(\".pdf\"):\n",
        "                loader = PyPDFLoader(file_path)\n",
        "            elif file_path.endswith(\".pptx\"):\n",
        "                loader = UnstructuredPowerPointLoader(file_path)\n",
        "            else:\n",
        "                print(f\"Warning: Unsupported file format for {file_path}\")\n",
        "                continue\n",
        "\n",
        "            pages = loader.load()\n",
        "            file_text = \"\\n\".join([page.page_content for page in pages])\n",
        "            all_text.append(f\"--- Content from {os.path.basename(file_path)} ---\\n{file_text}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return \"\\n\\n\".join(all_text)"
      ],
      "metadata": {
        "id": "_puMe-H7IKXC"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add summary\n",
        "def inject_summary(material: str, lecturer_summary: str = None) -> str:\n",
        "    # Don't inject the summary into the material itself\n",
        "    # The summary will be used in the question generation prompt instead\n",
        "    return material"
      ],
      "metadata": {
        "id": "eRbv2BBMJBaU"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chunking and summarization\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        ")\n",
        "\n",
        "def chunk_content(text: str) -> list[str]:\n",
        "    return text_splitter.split_text(text)"
      ],
      "metadata": {
        "id": "lqhHr0nrJF41"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Node 4: Question Generator Node\n",
        "\n",
        "question_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"chunk\"],\n",
        "    template=\"\"\"\n",
        "You are an exam generator AI. Based on the following content, generate 2 multiple-choice questions:\n",
        "\n",
        "---\n",
        "{chunk}\n",
        "---\n",
        "\n",
        "Generate questions that:\n",
        "- Test understanding of specific concepts, definitions, formulas, and processes\n",
        "- Ask directly about the subject matter without referencing \"the material\", \"the text\", or \"the lecture\"\n",
        "- Include 1 correct answer and 3 plausible, challenging distractors\n",
        "- Are clearly worded and academically appropriate\n",
        "\n",
        "AVOID phrases like:\n",
        "- \"according to the material\"\n",
        "- \"in the given text\"\n",
        "- \"the lecture states\"\n",
        "- \"based on the material\"\n",
        "- \"the document mentions\"\n",
        "\n",
        "Instead, ask direct questions about the concepts themselves.\n",
        "\n",
        "Format each question EXACTLY like this:\n",
        "Q: [Direct question about the concept]\n",
        "A) Option A\n",
        "B) Option B\n",
        "C) Option C\n",
        "D) Option D\n",
        "Correct Answer: [A/B/C/D]\n",
        "Explanation: [Short explanation of why this answer is correct]\n",
        "\n",
        "---\n",
        "\n",
        "Q: [Second direct question about the concept]\n",
        "A) Option A\n",
        "B) Option B\n",
        "C) Option C\n",
        "D) Option D\n",
        "Correct Answer: [A/B/C/D]\n",
        "Explanation: [Short explanation of why this answer is correct]\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "def generate_questions(chunk: str, summary: str = None) -> str:\n",
        "    # Add summary guidance to the prompt without including it in the content\n",
        "    summary_instruction = \"\"\n",
        "    if summary:\n",
        "        summary_instruction = f\"\\nIMPORTANT: {summary}\\n\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an exam generator AI. Based on the following content, generate 2 multiple-choice questions:\n",
        "\n",
        "---\n",
        "{chunk}\n",
        "---\n",
        "{summary_instruction}\n",
        "Generate questions that:\n",
        "- Test understanding of specific concepts, definitions, formulas, and processes\n",
        "- Ask directly about the subject matter without referencing \"the material\", \"the text\", or \"the lecture\"\n",
        "- Include 1 correct answer and 3 plausible, challenging distractors\n",
        "- Are clearly worded and academically appropriate\n",
        "\n",
        "AVOID phrases like:\n",
        "- \"according to the material\"\n",
        "- \"in the given text\"\n",
        "- \"the lecture states\"\n",
        "- \"based on the material\"\n",
        "- \"the document mentions\"\n",
        "\n",
        "Instead, ask direct questions about the concepts themselves.\n",
        "\n",
        "Format each question EXACTLY like this:\n",
        "Q: [Direct question about the concept]\n",
        "A) Option A\n",
        "B) Option B\n",
        "C) Option C\n",
        "D) Option D\n",
        "Correct Answer: [A/B/C/D]\n",
        "Explanation: [Short explanation of why this answer is correct]\n",
        "\n",
        "---\n",
        "\n",
        "Q: [Second direct question about the concept]\n",
        "A) Option A\n",
        "B) Option B\n",
        "C) Option C\n",
        "D) Option D\n",
        "Correct Answer: [A/B/C/D]\n",
        "Explanation: [Short explanation of why this answer is correct]\n",
        "\"\"\"\n",
        "\n",
        "    result = llm.invoke(prompt)\n",
        "    # Fix: Extract content from AIMessage object\n",
        "    if hasattr(result, 'content'):\n",
        "        return result.content\n",
        "    else:\n",
        "        return str(result)"
      ],
      "metadata": {
        "id": "vyLeAt34JL6J"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retry Logic Node\n",
        "\n",
        "def retry_generation(state):\n",
        "    if not state[\"raw_outputs\"]:\n",
        "        raise ValueError(\"Empty output. Retry generation.\")\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "pCExzBfoJQ3L"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subject Classification Node\n",
        "\n",
        "def classify_subject(material: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "You are a classifier AI. Based on the following content, identify the academic subject (e.g., Biology, Economics, Physics, Computer Science, Project Management, etc.):\n",
        "\n",
        "{material[:2000]}...\n",
        "\n",
        "Respond with just the subject name.\n",
        "\"\"\"\n",
        "    result = llm.invoke(prompt)\n",
        "    # Fix: Extract content from AIMessage object\n",
        "    if hasattr(result, 'content'):\n",
        "        return result.content.strip()\n",
        "    else:\n",
        "        return str(result).strip()"
      ],
      "metadata": {
        "id": "XFY-iqaYJVKu"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Output Formatter Node\n",
        "\n",
        "def format_output(raw_outputs: list[str]) -> dict:\n",
        "    questions = []\n",
        "    for output in raw_outputs:\n",
        "        # Split by \"Q:\" and process each question\n",
        "        parts = output.split(\"Q:\")\n",
        "        for part in parts[1:]:  # Skip first empty part\n",
        "            if part.strip():\n",
        "                questions.append(f\"Q:{part.strip()}\")\n",
        "\n",
        "    # Limit to 20 questions and ensure they're properly formatted\n",
        "    final_questions = []\n",
        "    for q in questions[:20]:\n",
        "        if \"Correct Answer:\" in q and \"Explanation:\" in q:\n",
        "            final_questions.append(q)\n",
        "\n",
        "    return {\"questions\": final_questions}"
      ],
      "metadata": {
        "id": "xLdlHXjxJdka"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced PDF Export Nodes\n",
        "\n",
        "def create_exam_header(pdf, course_info):\n",
        "    \"\"\"Create a professional header for the exam\"\"\"\n",
        "    pdf.set_font(\"Arial\", \"B\", 16)\n",
        "    pdf.cell(0, 10, f\"{course_info['course_code']}: {course_info['course_name']}\", ln=True, align='C')\n",
        "    pdf.ln(5)\n",
        "\n",
        "    pdf.set_font(\"Arial\", \"B\", 14)\n",
        "    pdf.cell(0, 10, \"EXAMINATION\", ln=True, align='C')\n",
        "    pdf.ln(5)\n",
        "\n",
        "    pdf.set_font(\"Arial\", \"\", 12)\n",
        "    pdf.cell(0, 8, f\"Instructor: {course_info['lecturer_name']}\", ln=True)\n",
        "    pdf.cell(0, 8, f\"Date: {datetime.now().strftime('%B %d, %Y')}\", ln=True)\n",
        "    pdf.ln(10)\n",
        "\n",
        "    pdf.set_font(\"Arial\", \"B\", 12)\n",
        "    pdf.cell(0, 8, \"Instructions: Choose the best answer for each question.\", ln=True)\n",
        "    pdf.ln(10)\n",
        "\n",
        "def export_questions_only_pdf(questions: list[str], course_info: dict, filename=\"exam_questions.pdf\") -> str:\n",
        "    \"\"\"Export PDF with questions only (no answers or explanations)\"\"\"\n",
        "    pdf = FPDF()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "    pdf.add_page()\n",
        "\n",
        "    # Add header\n",
        "    create_exam_header(pdf, course_info)\n",
        "\n",
        "    pdf.set_font(\"Arial\", \"\", 12)\n",
        "\n",
        "    question_num = 1\n",
        "    for q in questions:\n",
        "        lines = q.split('\\n')\n",
        "\n",
        "        # Process each question\n",
        "        for line in lines:\n",
        "            if line.startswith('Q:'):\n",
        "                pdf.set_font(\"Arial\", \"B\", 12)\n",
        "                question_text = line[2:].strip()\n",
        "                try:\n",
        "                    pdf.multi_cell(0, 8, f\"{question_num}. {question_text}\", 0, 'L')\n",
        "                except:\n",
        "                    pdf.multi_cell(0, 8, f\"{question_num}. {question_text.encode('ascii', 'replace').decode('ascii')}\", 0, 'L')\n",
        "                pdf.ln(3)\n",
        "                pdf.set_font(\"Arial\", \"\", 12)\n",
        "                question_num += 1\n",
        "\n",
        "            elif line.strip().startswith(('A)', 'B)', 'C)', 'D)')):\n",
        "                try:\n",
        "                    pdf.multi_cell(0, 6, f\"   {line.strip()}\", 0, 'L')\n",
        "                except:\n",
        "                    pdf.multi_cell(0, 6, f\"   {line.strip().encode('ascii', 'replace').decode('ascii')}\", 0, 'L')\n",
        "\n",
        "            elif line.startswith('Correct Answer:') or line.startswith('Explanation:'):\n",
        "                break  # Stop processing when we hit answers/explanations\n",
        "\n",
        "        pdf.ln(5)\n",
        "\n",
        "    output_path = os.path.join(\"./\", filename)\n",
        "    pdf.output(output_path)\n",
        "    return output_path\n",
        "\n",
        "def export_full_pdf(questions: list[str], course_info: dict, filename=\"exam_with_answers.pdf\") -> str:\n",
        "    \"\"\"Export PDF with questions, answers, and explanations\"\"\"\n",
        "    pdf = FPDF()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "    pdf.add_page()\n",
        "\n",
        "    # Add header\n",
        "    create_exam_header(pdf, course_info)\n",
        "\n",
        "    pdf.set_font(\"Arial\", \"\", 12)\n",
        "\n",
        "    question_num = 1\n",
        "    for q in questions:\n",
        "        lines = q.split('\\n')\n",
        "\n",
        "        # Process each question\n",
        "        for line in lines:\n",
        "            if line.startswith('Q:'):\n",
        "                pdf.set_font(\"Arial\", \"B\", 12)\n",
        "                question_text = line[2:].strip()\n",
        "                try:\n",
        "                    pdf.multi_cell(0, 8, f\"{question_num}. {question_text}\", 0, 'L')\n",
        "                except:\n",
        "                    pdf.multi_cell(0, 8, f\"{question_num}. {question_text.encode('ascii', 'replace').decode('ascii')}\", 0, 'L')\n",
        "                pdf.ln(3)\n",
        "                pdf.set_font(\"Arial\", \"\", 12)\n",
        "                question_num += 1\n",
        "\n",
        "            elif line.strip().startswith(('A)', 'B)', 'C)', 'D)')):\n",
        "                try:\n",
        "                    pdf.multi_cell(0, 6, f\"   {line.strip()}\", 0, 'L')\n",
        "                except:\n",
        "                    pdf.multi_cell(0, 6, f\"   {line.strip().encode('ascii', 'replace').decode('ascii')}\", 0, 'L')\n",
        "\n",
        "            elif line.startswith('Correct Answer:'):\n",
        "                pdf.set_font(\"Arial\", \"B\", 11)\n",
        "                try:\n",
        "                    pdf.multi_cell(0, 6, f\"   {line.strip()}\", 0, 'L')\n",
        "                except:\n",
        "                    pdf.multi_cell(0, 6, f\"   {line.strip().encode('ascii', 'replace').decode('ascii')}\", 0, 'L')\n",
        "                pdf.set_font(\"Arial\", \"\", 12)\n",
        "\n",
        "            elif line.startswith('Explanation:'):\n",
        "                pdf.set_font(\"Arial\", \"I\", 11)\n",
        "                try:\n",
        "                    pdf.multi_cell(0, 6, f\"   {line.strip()}\", 0, 'L')\n",
        "                except:\n",
        "                    pdf.multi_cell(0, 6, f\"   {line.strip().encode('ascii', 'replace').decode('ascii')}\", 0, 'L')\n",
        "                pdf.set_font(\"Arial\", \"\", 12)\n",
        "\n",
        "        pdf.ln(8)\n",
        "\n",
        "    output_path = os.path.join(\"./\", filename)\n",
        "    pdf.output(output_path)\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "qK2uPSRnJg1t"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Enhanced LangGraph\n",
        "\n",
        "from typing import TypedDict, List\n",
        "\n",
        "class ExamState(TypedDict):\n",
        "    file_paths: List[str]\n",
        "    lecturer_name: str\n",
        "    course_code: str\n",
        "    course_name: str\n",
        "    summary: str\n",
        "    text: str\n",
        "    chunks: List[str]\n",
        "    raw_outputs: List[str]\n",
        "    questions: List[str]\n",
        "    questions_only_pdf: str\n",
        "    full_pdf: str\n",
        "    subject: str\n",
        "\n",
        "def build_exam_graph():\n",
        "    workflow = StateGraph(ExamState)\n",
        "\n",
        "    workflow.add_node(\"load_files\", RunnableLambda(lambda state: {\"text\": load_documents(state[\"file_paths\"])}))\n",
        "\n",
        "    workflow.add_node(\"add_summary\", RunnableLambda(lambda state: {\"text\": inject_summary(state[\"text\"], state.get(\"summary\"))}))\n",
        "\n",
        "    workflow.add_node(\"classify\", RunnableLambda(lambda state: {\"subject\": classify_subject(state[\"text\"])}))\n",
        "\n",
        "    workflow.add_node(\"chunk\", RunnableLambda(lambda state: {\"chunks\": chunk_content(state[\"text\"])}))\n",
        "\n",
        "    workflow.add_node(\"generate\", RunnableLambda(lambda state: {\n",
        "        \"raw_outputs\": [generate_questions(chunk, state.get(\"summary\")) for chunk in state[\"chunks\"][:10]]  # Pass summary to generation\n",
        "    }))\n",
        "\n",
        "    workflow.add_node(\"retry\", RunnableLambda(retry_generation))\n",
        "\n",
        "    workflow.add_node(\"format\", RunnableLambda(lambda state: format_output(state[\"raw_outputs\"])))\n",
        "\n",
        "    workflow.add_node(\"export_pdfs\", RunnableLambda(lambda state: {\n",
        "        \"course_info\": {\n",
        "            \"lecturer_name\": state[\"lecturer_name\"],\n",
        "            \"course_code\": state[\"course_code\"],\n",
        "            \"course_name\": state[\"course_name\"]\n",
        "        },\n",
        "        \"questions_only_pdf\": export_questions_only_pdf(\n",
        "            state[\"questions\"],\n",
        "            {\n",
        "                \"lecturer_name\": state[\"lecturer_name\"],\n",
        "                \"course_code\": state[\"course_code\"],\n",
        "                \"course_name\": state[\"course_name\"]\n",
        "            },\n",
        "            f\"{state['course_code']}_exam_questions.pdf\"\n",
        "        ),\n",
        "        \"full_pdf\": export_full_pdf(\n",
        "            state[\"questions\"],\n",
        "            {\n",
        "                \"lecturer_name\": state[\"lecturer_name\"],\n",
        "                \"course_code\": state[\"course_code\"],\n",
        "                \"course_name\": state[\"course_name\"]\n",
        "            },\n",
        "            f\"{state['course_code']}_exam_with_answers.pdf\"\n",
        "        ),\n",
        "        \"questions\": state[\"questions\"],\n",
        "        \"subject\": state[\"subject\"]\n",
        "    }))\n",
        "\n",
        "    # Define edges\n",
        "    workflow.set_entry_point(\"load_files\")\n",
        "    workflow.add_edge(\"load_files\", \"add_summary\")\n",
        "    workflow.add_edge(\"add_summary\", \"classify\")\n",
        "    workflow.add_edge(\"classify\", \"chunk\")\n",
        "    workflow.add_edge(\"chunk\", \"generate\")\n",
        "    workflow.add_edge(\"generate\", \"retry\")\n",
        "    workflow.add_edge(\"retry\", \"format\")\n",
        "    workflow.add_edge(\"format\", \"export_pdfs\")\n",
        "    workflow.add_edge(\"export_pdfs\", END)\n",
        "\n",
        "    return workflow.compile()"
      ],
      "metadata": {
        "id": "Y7aAYmb6JlP6"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Run\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    graph = build_exam_graph()\n",
        "\n",
        "    input_data = {\n",
        "        \"file_paths\": [\"5.pdf\", \"6.pdf\",\"4.pdf\"],  # List of multiple files\n",
        "        \"lecturer_name\": \"Dr. John Bush Idoko\",\n",
        "        \"course_code\": \"ECC427\",\n",
        "        \"course_name\": \"Business for Engineers\",\n",
        "        \"summary\": \"Focus on questions that test student understanding and not ability to remembr facts\"\n",
        "    }\n",
        "\n",
        "    result = graph.invoke(input_data)\n",
        "    print(f\"\\nCourse: {input_data['course_code']} - {input_data['course_name']}\")\n",
        "    print(f\"Instructor: {input_data['lecturer_name']}\")\n",
        "    print(f\"Subject: {result['subject']}\")\n",
        "    print(f\"\\nQuestions-only PDF: {result['questions_only_pdf']}\")\n",
        "    print(f\"Full PDF with answers: {result['full_pdf']}\")\n",
        "    print(f\"\\nGenerated {len(result['questions'])} questions total\")\n",
        "\n",
        "    # Display first few questions as preview\n",
        "    for idx, q in enumerate(result['questions'][:3], start=1):\n",
        "        print(f\"\\nPreview Question {idx}:\")\n",
        "        lines = q.split('\\n')\n",
        "        for line in lines:\n",
        "            if not line.startswith('Correct Answer:') and not line.startswith('Explanation:'):\n",
        "                print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz2OfOkXK35S",
        "outputId": "5a6c9c31-4040-4a59-b616-0d1a754b4444"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Course: ECC427 - Business for Engineers\n",
            "Instructor: Dr. John Bush Idoko\n",
            "Subject: Industrial Engineering\n",
            "\n",
            "Questions-only PDF: ./ECC427_exam_questions.pdf\n",
            "Full PDF with answers: ./ECC427_exam_with_answers.pdf\n",
            "\n",
            "Generated 20 questions total\n",
            "\n",
            "Preview Question 1:\n",
            "Q:What is the primary focus of a Plant Engineer's responsibilities in a production activity?\n",
            "A) Designing and implementing changes to the product\n",
            "B) Running the support services of the plant and its activities\n",
            "C) Conducting maintenance of the plant equipment\n",
            "D) Managing the overall production process and strategy\n",
            "\n",
            "\n",
            "Preview Question 2:\n",
            "Q:What is the key distinction between the roles of a Design Engineer and a Maintenance Engineer in a production plant?\n",
            "A) The Design Engineer focuses on product design, while the Maintenance Engineer focuses on plant equipment maintenance\n",
            "B) The Design Engineer is responsible for maintenance, while the Maintenance Engineer designs new equipment\n",
            "C) The Design Engineer specifies requirements for changes, while the Maintenance Engineer implements those changes\n",
            "D) The Design Engineer designs and implements small changes to the plant, while the Maintenance Engineer designs and specifies maintenance criteria\n",
            "\n",
            "\n",
            "Preview Question 3:\n",
            "Q:What is the primary objective of facility planning in the context of manufacturing facilities?\n",
            "A) To optimize production equipment and reduce costs\n",
            "B) To determine how the manufacturing facility best supports production\n",
            "C) To improve quality control and inspection processes\n",
            "D) To increase the size of the production areas\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sPdN9t5hL3N9"
      },
      "execution_count": 94,
      "outputs": []
    }
  ]
}